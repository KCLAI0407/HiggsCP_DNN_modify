{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f262e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from Eearly_stop import *\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import sys\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from src_py.cpmix_original_utils import preprocess_data\n",
    "from src_py.cpmix_utils import preprocess_data       ##with Bkgd\n",
    "from src_py.rhorho import RhoRhoEvent\n",
    "from src_py.a1a1 import A1A1Event\n",
    "from src_py.a1rho import A1RhoEvent\n",
    "from src_py.data_utils import read_np, EventDatasets\n",
    "from src_py.process_background import convert_bkgd_raw\n",
    "import train_rhorho, train_a1rho, train_a1a1\n",
    "from src_py.metrics_utils import calculate_deltas_unsigned, calculate_deltas_signed\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:7') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f3b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "types = {\"nn_rhorho\": train_rhorho.start,\"nn_a1rho\": train_a1rho.start,\"nn_a1a1\": train_a1a1.start}\n",
    "parser = argparse.ArgumentParser(description='Train classifier')\n",
    "\n",
    "decaymodes = ['rhorho', 'a1rho', 'a1a1']\n",
    "decaymode = decaymodes[0]  ###### Change this to corresponding decaymode ############\n",
    "\n",
    "parser.add_argument(\"-i\", \"--input\", dest=\"IN\", default='HiggsCP_data/'+decaymode+'_bkgd')\n",
    "parser.add_argument(\"-t\", \"--type\", dest=\"TYPE\", choices=types.keys(), default='nn_'+ decaymode)\n",
    "\n",
    "parser.add_argument(\"--num_classes\", dest=\"NUM_CLASSES\", type=int, default=21)\n",
    "parser.add_argument(\"-l\", \"--layers\", dest=\"LAYERS\", type=int, help = \"number of NN layers\", default=6)\n",
    "parser.add_argument(\"-s\", \"--size\", dest=\"SIZE\", type=int, help=\"NN size\", default=1000)\n",
    "parser.add_argument(\"-lambda\", \"--lambda\", type=float, dest=\"LAMBDA\", help=\"value of lambda parameter\", default=0.0)\n",
    "parser.add_argument(\"-m\", \"--method\", dest=\"METHOD\", choices=[\"A\", \"B\", \"C\"], default=\"A\")\n",
    "parser.add_argument(\"-o\", \"--optimizer\", dest=\"OPT\", \n",
    "    choices=[\"GradientDescentOptimizer\", \"AdadeltaOptimizer\", \"AdagradOptimizer\",\n",
    "         \"ProximalAdagradOptimizer\", \"AdamOptimizer\", \"FtrlOptimizer\",\n",
    "         \"ProximalGradientDescentOptimizer\", \"RMSPropOptimizer\"], default=\"AdamOptimizer\")\n",
    "parser.add_argument(\"-d\", \"--dropout\", dest=\"DROPOUT\", type=float, default=0.0)\n",
    "parser.add_argument(\"-e\", \"--epochs\", dest=\"EPOCHS\", type=int, default=25)\n",
    "# parser.add_argument(\"-f\", \"--features\", dest=\"FEAT\", help=\"Features\", default=\"Variant-All\")\n",
    "# #         choices= [\"Variant-All\", \"Variant-1.0\", \"Variant-1.1\", \"Variant-2.0\", \"Variant-2.1\",\n",
    "# #                   \"Variant-2.2\", \"Variant-3.0\", \"Variant-3.1\", \"Variant-4.0\", \"Variant-4.1\"])\n",
    "\n",
    "parser.add_argument(\"--miniset\", dest=\"MINISET\", type=lambda s: s.lower() in ['true', 't', 'yes', '1'], default=False)\n",
    "parser.add_argument(\"--z_noise_fraction\", dest=\"Z_NOISE_FRACTION\", type=float, default=0.0)\n",
    "\n",
    "parser.add_argument(\"--delt_classes\", dest=\"DELT_CLASSES\", type=int, default=0,\n",
    "                    help='Maximal distance between predicted and valid class for event being considered as correctly classified')\n",
    "\n",
    "parser.add_argument(\"--unweighted\", dest=\"UNWEIGHTED\", type=lambda s: s.lower() in ['true', 't', 'yes', '1'], default=False)\n",
    "parser.add_argument(\"--reuse_weights\", dest=\"REUSE_WEIGHTS\", type=bool, default=False)\n",
    "parser.add_argument(\"--restrict_most_probable_angle\", dest=\"RESTRICT_MOST_PROBABLE_ANGLE\", type=bool, default=False)\n",
    "parser.add_argument(\"--force_download\", dest=\"FORCE_DOWNLOAD\", type=bool, default=False)\n",
    "parser.add_argument(\"--normalize_weights\", dest=\"NORMALIZE_WEIGHTS\", type=bool, default=False)\n",
    "\n",
    "\n",
    "parser.add_argument(\"--beta\",  type=float, dest=\"BETA\", help=\"value of beta parameter for polynomial smearing\", default=0.0)\n",
    "parser.add_argument(\"--pol_b\", type=float, dest=\"pol_b\", help=\"value of b parameter for polynomial smearing\", default=0.0)\n",
    "parser.add_argument(\"--pol_c\", type=float, dest=\"pol_c\", help=\"value of c parameter for polynomial smearing\", default=0.0)\n",
    "\n",
    "parser.add_argument(\"--w1\", dest=\"W1\")\n",
    "parser.add_argument(\"--w2\", dest=\"W2\")\n",
    "parser.add_argument(\"--f\", dest=\"FEAT\", default=\"Variant-All\")\n",
    "parser.add_argument(\"--plot_features\", dest=\"PLOT_FEATURES\", choices=[\"NO\", \"FILTER\", \"NO-FILTER\"], default=\"NO\")\n",
    "parser.add_argument(\"--training_method\", dest=\"TRAINING_METHOD\", choices=[\"soft_weights\", \"soft_c012s\",  \"soft_argmaxs\", \"regr_c012s\", \"regr_weights\", \"regr_argmaxs\"], default=\"soft_weights\")\n",
    "parser.add_argument(\"--hits_c012s\", dest=\"HITS_C012s\", choices=[\"hits_c0s\", \"hits_c1s\",  \"hits_c2s\"], default=\"hits_c0s\")\n",
    "\n",
    "parser.add_argument(\"-r\", \"--reprocess\", dest=\"REPRO\", type=bool, default=True)\n",
    "args, unknown = parser.parse_known_args()\n",
    "parser.add_argument(\"-bkgd\", \"--bkgdpath\", dest=\"BKGDPATH\", default= 'Ztt_dataset_Elz/pythia.Z_115_135.%s.1M.*.outTUPLE_labFrame')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "parser.add_argument(\"--label_bkgd\", dest=\"LABEL_BKGD\", type=bool, default=False)\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a585e",
   "metadata": {},
   "source": [
    "# Preprocessing signal samples from all the decaymodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "#     for decaymode in tqdm(decaymodes):\n",
    "    args.IN = 'HiggsCP_data/'+decaymode\n",
    "    args.TYPE = 'nn_'+decaymode\n",
    "    data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "    event = eval(events[args.TYPE])(data, args)\n",
    "    points = EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED)\n",
    "    pickle.dump(points,open(args.IN+'/events_wo_background21.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d7a5e",
   "metadata": {},
   "source": [
    "## with bkgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b79f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "    args.LABEL_BKGD = True\n",
    "    args.Z_NOISE_FRACTION = 0.8\n",
    "    args.IN = 'HiggsCP_data/'+decaymode+'_bkgd'\n",
    "    args.TYPE = 'nn_'+decaymode\n",
    "    data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "    event = eval(events[args.TYPE])(data, args)\n",
    "    points = EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED)\n",
    "    pickle.dump(points,open(args.IN+'/events_w_background.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d34dc8",
   "metadata": {},
   "source": [
    "# Loading signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "points=pickle.load(open(args.IN+'/events_wo_background21.pk','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb79a67",
   "metadata": {},
   "source": [
    "## with bkgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f38c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "points=pickle.load(open(args.IN+'/events_w_background.pk','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a188a",
   "metadata": {},
   "source": [
    "# Training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, rhorho_data_mc,rhorho_data_true,rhorho_labels_mc,rhorho_labels_true):\n",
    "        self.rhorho_data_mc = torch.from_numpy(rhorho_data_mc).float().to(device)\n",
    "        self.rhorho_data_true = torch.from_numpy(rhorho_data_true).float().to(device)\n",
    "        \n",
    "        self.rhorho_labels_mc =torch.from_numpy(rhorho_labels_mc).float().to(device)\n",
    "        self.rhorho_labels_true =torch.from_numpy(rhorho_labels_true).float().to(device)\n",
    "    def __getitem__(self, index):\n",
    "        return self.rhorho_data_mc[index],self.rhorho_data_true[index],self.rhorho_labels_mc[index],self.rhorho_labels_true[index]\n",
    "    def __len__(self):\n",
    "        return min(len(self.rhorho_labels_mc),len(self.rhorho_labels_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_train_idx=np.random.choice(np.arange(points.train.x.shape[0]),int(points.train.x.shape[0]*0.5),replace=False)\n",
    "true_train_idx=list(set(np.arange(points.train.x.shape[0]))-set(mc_train_idx))\n",
    "\n",
    "mc_valid_idx=np.random.choice(np.arange(points.valid.x.shape[0]),int(points.valid.x.shape[0]*0.5),replace=False)\n",
    "true_valid_idx=list(set(np.arange(points.valid.x.shape[0]))-set(mc_train_idx))\n",
    "\n",
    "mc_test_idx=np.random.choice(np.arange(points.test.x.shape[0]),int(points.test.x.shape[0]*0.5),replace=False)\n",
    "true_test_idx=list(set(np.arange(points.test.x.shape[0]))-set(mc_train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae75c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty=0.0\n",
    "\n",
    "train_datasets = MyDataset(points.train.x[mc_train_idx], points.train.x[true_train_idx]+uncertainty*np.random.normal(0,1,size=points.train.x[true_train_idx].shape),\n",
    "                          points.train.weights[mc_train_idx],points.train.weights[true_train_idx])\n",
    "train_loader = DataLoader(dataset = train_datasets,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "\n",
    "valid_datasets = MyDataset(points.valid.x[mc_valid_idx], points.valid.x[true_valid_idx]+uncertainty*np.random.normal(0,1,size=points.valid.x[true_valid_idx].shape),\n",
    "                          points.valid.weights[mc_valid_idx],points.valid.weights[true_valid_idx])\n",
    "valid_loader = DataLoader(dataset = valid_datasets,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "\n",
    "test_datasets = MyDataset(points.test.x[mc_test_idx], points.test.x[true_test_idx]+uncertainty*np.random.normal(0,1,size=points.test.x[true_test_idx].shape),\n",
    "                          points.test.weights[mc_test_idx],points.test.weights[true_test_idx])\n",
    "test_loader = DataLoader(dataset = test_datasets,batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40371cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, num_layers=1, size=100, lr=1e-3, drop_prob=0, inplace=False, \n",
    "                 tloss=\"regr_weights\", activation='linear', input_noise=0.0, optimizer=\"AdamOptimizer\"):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_features,size,bias=False)\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.extend([nn.Linear(size,size,bias=False),\n",
    "                           nn.BatchNorm1d(size),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(drop_prob, inplace)\n",
    "                          ])\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "        self.linear2 = nn.Linear(size,num_classes,bias=False)\n",
    "        self.linear3 = nn.Linear(size,2,bias=False)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        out = self.linear2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(os.getcwd() + '/model')\n",
    "if not os.path.exists(model_path): \n",
    "    os.mkdir(os.path.join(model_path))\n",
    "model = NeuralNetwork(num_features=points.train.x.shape[1], num_classes=args.NUM_CLASSES+1,num_layers=args.LAYERS,drop_prob=0).to(device)\n",
    "# model = NeuralNetwork(num_features=points[particle_idx].train.x.shape[1], num_classes=args.NUM_CLASSES,num_layers=args.LAYERS,drop_prob=0).to(device)\n",
    "opt_g=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "#early_stopping = EarlyStopping(patience=12, verbose=True,path=model_path+'/'+decaymode+'_best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e548ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=model_path+'/'+decaymode+'_the_best_model_'+str(args.NUM_CLASSES)+str(args.FEAT)+'.pt'):\n",
    "    print(\"=> saving checkpoint\")\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint):\n",
    "    print(\"=> loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    opt_g.load_state_dict(checkpoint['opt_g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(torch.load(model_path+'/'+decaymode+'_the_best_model_'+str(args.NUM_CLASSES)+str(args.FEAT)+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e517e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Eearly_stop\n",
    "epoch=200\n",
    "training_loss=[]\n",
    "validation_loss=[]\n",
    "tr_pred = []\n",
    "tr_true = []\n",
    "v_pred = []\n",
    "v_true = []\n",
    "\n",
    "checkpoint={'state_dict':model.state_dict(), 'opt_g':opt_g.state_dict()}\n",
    "save_checkpoint(checkpoint)\n",
    "\n",
    "with open('Results/TrainingOutputs/'+decaymode+'_TrainingOutputs_'+str(args.NUM_CLASSES)+'.txt','wb') as f:\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        train_loss,sample_numbers,acc,total_samples,bg_acc=0,0,0,0,0\n",
    "        for batch_idx, (rhorho_s,rhorho_t,label_s,_) in enumerate(train_loader):\n",
    "\n",
    "            opt_g.zero_grad()\n",
    "            rhorho_s=rhorho_s[label_s.sum(axis=1)!=0]\n",
    "            label_s=label_s[label_s.sum(axis=1)!=0]\n",
    "            outputs=model(rhorho_s)\n",
    "            training_outputs=model(rhorho_s).detach().cpu()\n",
    "            training_outputs=torch.softmax(torch.cat([training_outputs]),axis=1).numpy()\n",
    "            training_labels=label_s.cpu().numpy()\n",
    "            \n",
    "            if isinstance(criterion,nn.CrossEntropyLoss):\n",
    "                loss=criterion(outputs,torch.argmax(label_s,axis=1))\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                acc+=(predictions==torch.argmax(label_s,axis=1)).sum().item()\n",
    "            else:\n",
    "                loss=criterion(outputs,label_s)\n",
    "            loss.backward()\n",
    "            train_loss+=loss.item()*len(rhorho_s)\n",
    "            sample_numbers+=len(rhorho_s)\n",
    "            opt_g.step()\n",
    "            \n",
    "        print('\\r training loss: %.3f \\t acc: %.3f \\t' %(train_loss/sample_numbers,acc/sample_numbers),end='')\n",
    "        training_loss.append(train_loss/sample_numbers)\n",
    "        tr_pred.extend(training_outputs)\n",
    "        tr_true.extend(training_labels)\n",
    "        f.write(('\\r training loss: %.3f \\t acc: %.3f \\t ' %(train_loss/sample_numbers,acc/sample_numbers)).encode())\n",
    "        print()\n",
    "        \n",
    "        vaild_loss,vaild_acc,vaild_numbers,total_samples,bg_acc=0,0,0,0,0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (rhorho_s,rhorho_t,label_s,label_t) in enumerate(valid_loader):\n",
    "                total_samples+=len(rhorho_t)\n",
    "                rhorho_t= rhorho_t[label_t.sum(axis=1)!=0]\n",
    "                label_t = label_t[label_t.sum(axis=1)!=0]\n",
    "                valid_labels=label_t.cpu().numpy()\n",
    "                outputs=model(rhorho_t)\n",
    "                valid_outputs = model(rhorho_t).detach().cpu()\n",
    "                valid_outputs=torch.softmax(torch.cat([valid_outputs]),axis=1).numpy()\n",
    "                \n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                vaild_acc+=(predictions==torch.argmax(label_t,axis=1)).sum().item()\n",
    "                vaild_numbers+=len(rhorho_t)\n",
    "                if isinstance(criterion,nn.CrossEntropyLoss):\n",
    "                    loss=criterion(outputs,torch.argmax(label_t,axis=1))\n",
    "                else:\n",
    "                    loss=criterion(output,label_t)\n",
    "                vaild_loss+=loss.item()*len(rhorho_t)\n",
    "                \n",
    "        print()\n",
    "        print('\\r validation loss: %.3f \\t valid acc: %.3f \\t ' %(vaild_loss/vaild_numbers,vaild_acc/vaild_numbers),end='')\n",
    "        f.write(('\\r validation loss: %.3f \\t valid acc: %.3f \\t ' %(vaild_loss/vaild_numbers,vaild_acc/vaild_numbers)).encode())\n",
    "        print()\n",
    "        validation_loss.append(vaild_loss/vaild_numbers)\n",
    "        v_pred.extend(valid_outputs)\n",
    "        v_true.extend(valid_labels)\n",
    "        #early_stopping(-vaild_acc/vaild_numbers,model)\n",
    "        #if early_stopping.early_stop:\n",
    "            #print(\"Early stopping\")\n",
    "            #f.write((\"Early stopping\").encode())\n",
    "            #break;\n",
    "            # test_loss=0\n",
    "    # with torch.no_grad():\n",
    "    #     for inputs, label in test_loader:\n",
    "    #         outputs=model(inputs)\n",
    "    #         test_loss+=mse_loss(outputs,label).item()*len(inputs)\n",
    "    #     print('test loss: %f' %(test_loss/len(test_loader.dataset.tensors[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d900047",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['skyblue','orange']\n",
    "plt.plot(training_loss, color=colors[0],label='training loss')\n",
    "plt.plot(validation_loss, color=colors[1],label='validation_loss')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('loss vs epoch (signal background classification) with epoches = 200')\n",
    "TestResults_path = os.path.join(os.getcwd() + '/Results/TestResults/')\n",
    "if not os.path.exists(TestResults_path): \n",
    "    os.mkdir(os.path.join(TestResults_path))\n",
    "plt.savefig(TestResults_path+decaymode+'_loss.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee08932",
   "metadata": {},
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd6ec7",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trues_train = np.argmax(tr_true, axis=1)\n",
    "prediction_train = np.argmax(tr_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix_train = confusion_matrix(Trues_train, prediction_train)\n",
    "cf_matrix_train_n = cf_matrix_train.astype('float') / cf_matrix_train.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6daaf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_train_n = pd.DataFrame(cf_matrix_train_n)\n",
    "ax = plt.subplots(figsize=(12, 8), dpi=100)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "sns.heatmap(df_cm_train_n, annot=True, fmt='.2f', square=True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "TestResults_path = os.path.join(os.getcwd() + '/Results/TestResults/')\n",
    "if not os.path.exists(TestResults_path):\n",
    "    os.mkdir(os.path.join(TestResults_path))\n",
    "plt.savefig(TestResults_path+decaymode+'_train_confusion_matrix_norm.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7bab5e",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trues_valid = np.argmax(v_true, axis=1)\n",
    "prediction_valid = np.argmax(v_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix_valid = confusion_matrix(Trues_valid, prediction_valid)\n",
    "cf_matrix_valid_n = cf_matrix_valid.astype('float') / cf_matrix_valid.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8697f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_valid_n = pd.DataFrame(cf_matrix_valid_n)\n",
    "ax = plt.subplots(figsize=(12, 8), dpi=100)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "sns.heatmap(df_cm_valid_n, annot=True, fmt='.2f', square=True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "TestResults_path = os.path.join(os.getcwd() + '/Results/TestResults/')\n",
    "if not os.path.exists(TestResults_path):\n",
    "    os.mkdir(os.path.join(TestResults_path))\n",
    "plt.savefig(TestResults_path+decaymode+'_valid_confusion_matrix_norm.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307df0e",
   "metadata": {},
   "source": [
    "# Preprocessing singal and bkgd from all the decaymodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "    args.Z_NOISE_FRACTION = 1\n",
    "    args.IN = 'HiggsCP_data/'+decaymode\n",
    "    args.TYPE = 'nn_'+decaymode\n",
    "    data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "    event = eval(events[args.TYPE])(data, args)\n",
    "    points = EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED)\n",
    "    pickle.dump(points,open(args.IN+'/events_w_background21_test.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc2a44",
   "metadata": {},
   "source": [
    "## signal background classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "    args.LABEL_BKGD = False\n",
    "    args.Z_NOISE_FRACTION = 1\n",
    "    args.IN = 'HiggsCP_data/'+decaymode+'_bkgd'\n",
    "    args.TYPE = 'nn_'+decaymode\n",
    "    data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "    event = eval(events[args.TYPE])(data, args)\n",
    "    points = EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED)\n",
    "    pickle.dump(points,open(args.IN+'/events_w_background_test.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfaaa42",
   "metadata": {},
   "source": [
    "# Loading bkgd samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058dde3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_points=pickle.load(open(args.IN+'/events_w_background21.pk','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3193bb8",
   "metadata": {},
   "source": [
    "## signal background classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6641516",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_points=pickle.load(open(args.IN+'/events_w_background_test.pk','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945a91d",
   "metadata": {},
   "source": [
    "## loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecd049",
   "metadata": {},
   "outputs": [],
   "source": [
    "background=[]\n",
    "background.append(background_points.train.x[background_points.train.weights.sum(axis=1)==0])\n",
    "background.append(background_points.valid.x[background_points.valid.weights.sum(axis=1)==0])\n",
    "background.append(background_points.test.x[background_points.test.weights.sum(axis=1)==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32450bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "background=np.concatenate(background)\n",
    "print(background.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "background=torch.tensor(background).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1769d41",
   "metadata": {},
   "source": [
    "# Testing NN w/ bkgd only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs=[]\n",
    "    #for i in tqdm(range(0, 400000,batch_size)):\n",
    "    for i in tqdm(range(500000, 1000000,batch_size)):\n",
    "        outputs.append(model(background[i:i+batch_size]).detach().cpu())\n",
    "outputs=torch.cat(outputs)\n",
    "\n",
    "bg_outputs=torch.argmax(torch.softmax(outputs,axis=1),axis=1).numpy()\n",
    "bg_labels_counts=np.unique(bg_outputs,return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c55859",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bg_outputs,open(args.IN+'/NN_outputs_background_only.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c9af3",
   "metadata": {},
   "source": [
    "# Testing NN w/ signal only (Class 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f637cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    signal_outputs,signal_labels=[],[]\n",
    "    for batch_idx, (rhorho_s,rhorho_t,label_s,_) in enumerate(train_loader):\n",
    "        signal_outputs.append(model(rhorho_s).detach().cpu())\n",
    "        signal_labels.append(label_s.detach().cpu())\n",
    "signal_outputs=torch.softmax(torch.cat(signal_outputs),axis=1).numpy()\n",
    "signal_labels=np.concatenate(signal_labels)\n",
    "\n",
    "####### Filtering signal outputs that are classified to Class 0\n",
    "signal_outputs=signal_outputs[np.argmax(signal_labels,axis=1)==0]\n",
    "signal_labels=np.argmax(signal_outputs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(signal_outputs,open(args.IN+'/NN_outputs_signal_only.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508edf6",
   "metadata": {},
   "source": [
    "# Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee354a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_outputs = pickle.load(open(args.IN+'/NN_outputs_background_only.pk','rb'))\n",
    "signal_outputs= pickle.load(open(args.IN+'/NN_outputs_signal_only.pk','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a071703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe; enumerate is used to make column names\n",
    "columns=['Ztt','Signal']\n",
    "fig,ax=plt.subplots(dpi=150)\n",
    "\n",
    "df = pd.concat([pd.DataFrame(a, columns=[columns[i]]) for i, a in enumerate([bg_outputs, np.argmax(signal_outputs,axis=1)], 0)], axis=1)\n",
    "# plot the data\n",
    "#ax.set_xlim(0,args.NUM_CLASSES-1)\n",
    "ax.set_xlim(0,args.NUM_CLASSES)\n",
    "#ax = df.plot.hist(stacked=True, bins=args.NUM_CLASSES-1,ax=ax, color = ['skyblue','red']).get_figure()\n",
    "ax = df.plot.hist(stacked=True, bins=args.NUM_CLASSES,ax=ax, color = ['skyblue','red']).get_figure()\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Events\")\n",
    "# ax.set_xticks(np.arange(args.NUM_CLASSES-1))\n",
    "# ax.set_xticklabels((np.linspace(0,2,args.NUM_CLASSES-1)*np.pi))\n",
    "#bars = ax.patches\n",
    "# hatches = ['/','\\\\']\n",
    "\n",
    "# for i in range(2):\n",
    "#     for j in range(args.NUM_CLASSES-1):\n",
    "#         bars[i*(args.NUM_CLASSES-1)+j].set_hatch(hatches[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestResults_path = os.path.join(os.getcwd()) + '/Results/TestResults/'\n",
    "if not os.path.exists(TestResults_path): \n",
    "    os.mkdir(os.path.join(TestResults_path))\n",
    "ax.savefig(TestResults_path+decaymode+'_TestResults.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_df = pd.DataFrame(np.argmax(signal_outputs, axis=1), columns=[columns[1]])\n",
    "bkgd_df = pd.DataFrame(bg_outputs, columns=[columns[0]])\n",
    "signal_df = signal_df.groupby('Signal').size().to_frame('SgCounts').reset_index().rename({'Signal':'Class'},axis=1)\n",
    "bkgd_df = bkgd_df.groupby('Ztt').size().to_frame('BgCounts').reset_index().rename({'Ztt':'Class'},axis=1)\n",
    "total = pd.concat([signal_df,bkgd_df['BgCounts']],axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, axis = plt.subplots(2,1,figsize=(6,10),dpi=150)\n",
    "fig.suptitle( decaymode + \" Test Results\", fontsize=15)\n",
    "axis[0].set_title('Signal + Background Results', fontsize=11)\n",
    "axis[1].set_title('Background Results with Error Bar', fontsize=11)\n",
    "# set plot style: grey grid in the background:\n",
    "sns.set(style=\"dark\")\n",
    "\n",
    "total[['BgCounts','SgCounts']].plot(kind=\"bar\", ax = axis[0],stacked=True,color = ['skyblue','r']).get_figure()\n",
    "\n",
    "# add legend\n",
    "top_bar = mpatches.Patch(color='r', label='Signal')\n",
    "bottom_bar = mpatches.Patch(color='skyblue', label='Ztt')\n",
    "axis[0].legend(handles=[top_bar, bottom_bar])\n",
    "\n",
    "for i in range(len(axis)): \n",
    "    axis[i].set_xlabel(\"Classes\",fontsize=10)\n",
    "    axis[i].set_ylabel(\"Events\",fontsize=10)\n",
    "\n",
    "\n",
    "Poisson_std = [math.sqrt(i) for i in total['BgCounts'].to_numpy()]\n",
    "total[['BgCounts']].plot(kind=\"bar\", ax = axis[1],stacked=True,color = ['skyblue','r'], yerr = Poisson_std, alpha = 1)\n",
    "# ax.bar(x_pos, CTEs, yerr=error, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "axis[1].legend(handles=[bottom_bar])\n",
    "\n",
    "# show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c213d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestResults_path = os.path.join(os.getcwd() + '/Results/TestResults/')\n",
    "if not os.path.exists(TestResults_path): \n",
    "    os.mkdir(os.path.join(TestResults_path))\n",
    "fig.savefig(TestResults_path+decaymode+'_TestResults2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb396449",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Signal']\n",
    "fig,ax=plt.subplots(dpi=150)\n",
    "\n",
    "df = pd.concat([pd.DataFrame(a, columns=[columns[i]]) for i, a in enumerate([np.argmax(signal_outputs,axis=1)], 0)], axis=1)\n",
    "# plot the data\n",
    "ax.set_xlim(0,args.NUM_CLASSES)\n",
    "ax = df.plot.hist(stacked=True, bins=args.NUM_CLASSES,ax=ax, color = ['red']).get_figure()\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestResults_path = os.path.join(os.getcwd()) + '/Results/TestResults/'\n",
    "if not os.path.exists(TestResults_path): \n",
    "    os.mkdir(os.path.join(TestResults_path))\n",
    "ax.savefig(TestResults_path+decaymode+'_TestResults3.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bde986",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Ztt','Signal']\n",
    "fig,ax=plt.subplots(dpi=150)\n",
    "\n",
    "df = pd.concat([pd.DataFrame(a, columns=[columns[i]]) for i, a in enumerate([bg_outputs], 0)], axis=1)\n",
    "# plot the data\n",
    "ax.set_xlim(1,args.NUM_CLASSES-1)\n",
    "ax.set_ylim(0,1300)\n",
    "ax = df.plot.hist(stacked=True, bins=args.NUM_CLASSES,ax=ax, color = ['skyblue']).get_figure()\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestResults_path = os.path.join(os.getcwd()) + '/Results/TestResults/'\n",
    "if not os.path.exists(TestResults_path): \n",
    "    os.mkdir(os.path.join(TestResults_path))\n",
    "ax.savefig(TestResults_path+decaymode+'_TestResults4.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bkgd_df)\n",
    "print(signal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6581269",
   "metadata": {},
   "source": [
    "## confusion matrix for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = []\n",
    "t_true = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (rhorho_s,rhorho_t,label_s,label_t) in enumerate(test_loader):\n",
    "        \n",
    "        \n",
    "        outputs = model(rhorho_t).detach().cpu()\n",
    "        outputs=torch.softmax(torch.cat([outputs]),axis=1).numpy()\n",
    "        t_pred.extend(outputs)\n",
    "        \n",
    "        labels=label_t.cpu().numpy()\n",
    "        t_true.extend(labels)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269baa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trues_test = np.argmax(t_true, axis=1)\n",
    "prediction_test = np.argmax(t_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix_test = confusion_matrix(Trues_test, prediction_test)\n",
    "cf_matrix_test_n = cf_matrix_test.astype('float') / cf_matrix_test.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm_test_n = pd.DataFrame(cf_matrix_test_n)\n",
    "ax = plt.subplots(figsize=(12, 8), dpi=100)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "sns.heatmap(df_cm_test_n, annot=True, fmt='.2f', square=True, linewidths=.5, cmap=\"YlGnBu\")\n",
    "TestResults_path = os.path.join(os.getcwd() + '/Results/TestResults/')\n",
    "if not os.path.exists(TestResults_path):\n",
    "    os.mkdir(os.path.join(TestResults_path))\n",
    "plt.savefig(TestResults_path+decaymode+'_test_confusion_matrix_norm.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb22ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
